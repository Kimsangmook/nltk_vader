# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DPfXW5ZJUDsVUDP6POR1GTm8YL60eT8i
"""

import matplotlib.pyplot as plt
import networkx as nx
import pylab

import pandas as pd
import numpy as np

# VADER를 통한 감성 극성 분석
# operator and collections for calculating
#import operator
#from collections import Counter

# NLTK-VADER for sentiment analysis
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()

from google.colab import drive
drive.mount('/content/gdrive')

# 1. 크롤링한 파일을 읽기전용으로 호출함
file = open('/content/gdrive/My Drive/VTT_Sentiment/wikiPlotText.txt','r',encoding='utf-8')
lines = file.readlines()
lines = list(map(lambda s: s.strip(), lines))

# 2. 변수에 전체를 다시저장
plot =[]
for line in lines:
    plot.append(line)

sentiment_plot=[]
for line in lines:
    sentiment_plot.append(line)
file.close()
'''
while '' in plot:
 plot.remove('')
'''
plot=''.join(plot)
sentiment_plot=''.join(plot)
# 3. nltk
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

from nltk.tokenize import word_tokenize 
from nltk.tag import pos_tag


plot=plot.lower() #소문자만들기
plot=plot.split() #토큰화

from nltk.corpus import stopwords #불용어 제거
stop_words = set(stopwords.words('english')) #영어 불용어 가져오기
plot=[w for w in plot if not w in stop_words] #불용어 제거

from nltk.stem.snowball import SnowballStemmer#스테밍
stemmer = SnowballStemmer('english')
plot = [stemmer.stem(w) for w in plot]


from nltk.stem import WordNetLemmatizer #음소표기
wordnet_lemmatizer = WordNetLemmatizer()
plot = [wordnet_lemmatizer.lemmatize(w) for w in plot]

while 'gametitl' in plot:
 plot.remove('gametitl')

from nltk.tag import pos_tag
tagged_list=pos_tag(plot)

# NN 명사
nn_list = [t[0] for t in tagged_list if t[1] == "NN"]

# VB 동사

vb_list = [t[0] for t in tagged_list if t[1] == "VB"]

# JJ: 형용사

jj_list = [t[0] for t in tagged_list if t[1] == "JJ"]

from collections import Counter

# 6. 선별된 품사별 빈도수 계산 & 상위 빈도 10위 까지 출력
#명사
counts_nn = Counter(nn_list)
print(counts_nn.most_common(10))
#동사
counts_vb = Counter(vb_list)
print(counts_vb.most_common(10))
#형용사
counts_jj = Counter(jj_list)
print(counts_jj.most_common(10))

#명사
from wordcloud import WordCloud
cloud = WordCloud(width=600, height=500)
cloud.generate_from_frequencies(counts_nn)
plt.figure(figsize=(12,7))
plt.imshow(cloud)
plt.axis('off')

#동사
from wordcloud import WordCloud
cloud = WordCloud(width=600, height=500)
cloud.generate_from_frequencies(counts_vb)
plt.figure(figsize=(12,7))
plt.imshow(cloud)
plt.axis('off')

#형용사
from wordcloud import WordCloud
cloud = WordCloud(width=600, height=500)
cloud.generate_from_frequencies(counts_jj)
plt.figure(figsize=(12,7))
plt.imshow(cloud)
plt.axis('off')

pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()

def sentiment_analyzer_scores(sentence):
    score = analyser.polarity_scores(sentence)
    print("{}".format(str(score)))

sentiment_analyzer_scores (sentiment_plot)